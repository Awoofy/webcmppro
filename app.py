import os
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# OpenAI APIã®è¨­å®š
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.7,
)

# å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã®è¨­å®š
output_parser = StrOutputParser()

def create_chain(expert_type: str):
    """
    å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    
    Args:
        expert_type (str): å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ï¼ˆã‚³ãƒ¼ãƒ’ãƒ¼ or ãƒ¯ã‚¤ãƒ³ï¼‰
    
    Returns:
        Chain: è¨­å®šã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³
    """
    # å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®š
    if expert_type == "ãƒãƒªã‚¹ã‚¿(ã‚³ãƒ¼ãƒ’ãƒ¼)":
        system_prompt = """
        ã‚ãªãŸã¯ã‚³ãƒ¼ãƒ’ãƒ¼ã«é–¢ã™ã‚‹æ·±ã„çŸ¥è­˜ã‚’æŒã¤ãƒãƒªã‚¹ã‚¿ã§ã™ã€‚
        ã‚³ãƒ¼ãƒ’ãƒ¼ã®ç¨®é¡ã€ç„™ç…æ–¹æ³•ã€æŠ½å‡ºæ–¹æ³•ã€å‘³ã‚ã„ã€é¦™ã‚Šã€ç”£åœ°ãªã©ã€
        ã‚³ãƒ¼ãƒ’ãƒ¼ã«é–¢ã™ã‚‹ã‚ã‚‰ã‚†ã‚‹è³ªå•ã«å°‚é–€çš„ãªè¦³ç‚¹ã‹ã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
    else:  # ãƒ¯ã‚¤ãƒ³ã®å°‚é–€å®¶
        system_prompt = """
        ã‚ãªãŸã¯ãƒ¯ã‚¤ãƒ³ã«é–¢ã™ã‚‹æ·±ã„çŸ¥è­˜ã‚’æŒã¤ã‚½ãƒ ãƒªã‚¨ã§ã™ã€‚
        ãƒ¯ã‚¤ãƒ³ã®ç¨®é¡ã€ãƒ–ãƒ‰ã‚¦å“ç¨®ã€ç”Ÿç”£åœ°ã€å‘³ã‚ã„ã€é¦™ã‚Šã€ä¿å­˜æ–¹æ³•ã€
        æ–™ç†ã¨ã®ãƒšã‚¢ãƒªãƒ³ã‚°ãªã©ã€ãƒ¯ã‚¤ãƒ³ã«é–¢ã™ã‚‹ã‚ã‚‰ã‚†ã‚‹è³ªå•ã«å°‚é–€çš„ãªè¦³ç‚¹ã‹ã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])
    
    # ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    chain = prompt | llm | output_parser
    
    return chain

def get_llm_response(prompt: str, expert_type: str) -> str:
    """
    LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    
    Args:
        prompt (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        expert_type (str): å°‚é–€å®¶ã‚¿ã‚¤ãƒ—ï¼ˆã‚³ãƒ¼ãƒ’ãƒ¼ or ãƒ¯ã‚¤ãƒ³ï¼‰
    
    Returns:
        str: LLMã‹ã‚‰ã®å›ç­”
    """
    # ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
    chain = create_chain(expert_type)
    
    # ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ
    response = chain.invoke({
        "input": prompt,
        "chat_history": []  # å°†æ¥çš„ã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å®Ÿè£…ã™ã‚‹å ´åˆã«ä½¿ç”¨
    })
    
    return response

# Streamlitã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«è¨­å®š
st.title("å¤§äººãªé£²ã¿ç‰©å°‚é–€å®¶ã«è³ªå•ã—ã¦ã¿ã‚ˆã†ï¼ğŸ‘¨â€ğŸ³")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å°‚é–€å®¶é¸æŠç”¨ã®ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‚’é…ç½®
expert_type = st.sidebar.radio(
    "ã©ã¡ã‚‰ã«ç›¸è«‡ã—ã¾ã™ã‹ï¼Ÿï¼š",
    ["ãƒãƒªã‚¹ã‚¿(ã‚³ãƒ¼ãƒ’ãƒ¼)", "ã‚½ãƒ ãƒªã‚¨(ãƒ¯ã‚¤ãƒ³)"]
)

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã‚’é…ç½®
user_input = st.text_area(
    "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š",
    height=100,
    placeholder="ä¾‹ï¼šã‚³ãƒ¼ãƒ’ãƒ¼ã®å ´åˆï¼‰æµ…ç…ã‚Šã¨æ·±ç…ã‚Šã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ\nä¾‹ï¼šãƒ¯ã‚¤ãƒ³ã®å ´åˆï¼‰èµ¤ãƒ¯ã‚¤ãƒ³ã¨ç™½ãƒ¯ã‚¤ãƒ³ã®ä¿å­˜æ–¹æ³•ã®é•ã„ã¯ï¼Ÿ"
)

# é€ä¿¡ãƒœã‚¿ãƒ³
if st.button("è³ªå•ã™ã‚‹ï¼"):
    if user_input:
        try:
            # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤º
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—
                response = get_llm_response(user_input, expert_type)
                # å›ç­”ã‚’è¡¨ç¤º
                st.write("### å›ç­”ï¼š")
                st.write(response)
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{str(e)}")
    else:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")